{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6k28ebHySulj"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Initialize an empty list to store the data\n",
        "data = []\n",
        "\n",
        "# Step 2: Loop through the first 20 pages\n",
        "for page in range(1, 21):  # Loop from page 1 to 20\n",
        "    url = f'http://quotes.toscrape.com/page/{page}/'\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Check that the request was successful\n",
        "    # Step 3: Parse the HTML content of the page\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    # Step 4: Find all quote elements on the current page\n",
        "    quotes = soup.find_all('div', class_='quote')\n",
        "    # Step 5: Extract data and append it to the list\n",
        "    for quote in quotes:\n",
        "        text = quote.find('span', class_='text').get_text()\n",
        "        author = quote.find('small', class_='author').get_text()\n",
        "        tags = [tag.get_text() for tag in quote.find_all('a', class_='tag')]\n",
        "        data.append([text, author, ', '.join(tags)])\n",
        "\n",
        "# Step 6: Write the data to a CSV file using pandas\n",
        "df = pd.DataFrame(data, columns=['Quote', 'Author', 'Tags'])\n",
        "df.to_csv('quotes_20.csv', index=False, encoding='utf-8')\n",
        "\n",
        "print(\"Quotes from 20 pages have been successfully saved to 'quotes.csv'\")"
      ]
    }
  ]
}